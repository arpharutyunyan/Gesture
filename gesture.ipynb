{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.ops.gen_math_ops import mod\n",
    "from PIL import Image, ImageFilter\n",
    "import mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "NUM_CATEGORIES = 6\n",
    "TEST_SIZE = 0.5\n",
    "GESTURE = {0:\"ok\", 1:\"down\", 2:\"up\", 3:\"palm\", 4:\"fist\", 5:\"l\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    \"\"\"\n",
    "    Load image data from directory `data_dir`.\n",
    "    Assume `data_dir` has one directory named after each category, numbered\n",
    "    0 through NUM_CATEGORIES - 1. Inside each category directory will be some\n",
    "    number of image files.\n",
    "    Return tuple `(images, labels)`. `images` should be a list of all\n",
    "    of the images in the data directory, where each image is formatted as a\n",
    "    numpy ndarray with dimensions IMG_WIDTH x IMG_HEIGHT x 3. `labels` should\n",
    "    be a list of integer labels, representing the categories for each of the\n",
    "    corresponding `images`.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for dir in range(0, NUM_CATEGORIES):\n",
    "        # get path for each gesture like \"/home/arpine/Desktop/data/0\":  \n",
    "        d = os.path.join(data_dir, f\"{str(dir)}\")\n",
    "        # os.listdir(d) return the list of all names of images in that folder\n",
    "        for image_path in os.listdir(d):\n",
    "            # get the full path of specific image \n",
    "            full_path = os.path.join(data_dir, f\"{str(dir)}\", image_path)\n",
    "            # Returns an image that is loaded from the specified file\n",
    "            image = cv2.imread(full_path, )\n",
    "            # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            # cv2.imshow(\"im\", image)\n",
    "            # get dimension for each image\n",
    "            dim = (IMG_WIDTH, IMG_HEIGHT)\n",
    "            # resized the image\n",
    "            image_resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "            # add image and their directory name to images and labels list\n",
    "            images.append(image_resized)\n",
    "            labels.append(dir)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading ===========\n",
      "Images load time:  0:00:01.884775\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()  \n",
    "print(\"Loading ===========\") \n",
    "images, labels = load_data(\"/home/arpine/Desktop/test1\")   \n",
    "# images, labels = load_data(\"/home/arpine/Desktop/Gesture/test3\")  \n",
    "# images, labels = load_data(\"/home/arpine/Desktop/Gesture/poqr\")  \n",
    "# images, labels = load_data(\"/home/arpine/Desktop/Gesture/test (copy)\")\n",
    "finish_loading_time = datetime.now()\n",
    "print(\"Images load time: \", finish_loading_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"\n",
    "    Returns a compiled convolutional neural network model. Assume that the\n",
    "    `input_shape` of the first layer is `(IMG_WIDTH, IMG_HEIGHT, 3)`.\n",
    "    The output layer should have `NUM_CATEGORIES` units, one for each category.\n",
    "    \"\"\"\n",
    "    # Create a convolutional neural network\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "        # Convolutional layer. Learn 32 filters using a 3x3 kernel\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32, (5, 5), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "        ),\n",
    "        # Max-pooling layer, using 2x2 pool size\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            64, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "        ),\n",
    "        # Max-pooling layer, using 2x2 pool size\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            128, (3, 3), activation='relu', input_shape=((IMG_WIDTH), (IMG_HEIGHT), 3)\n",
    "        ),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            256, (3, 3), activation='relu', input_shape=((IMG_WIDTH), (IMG_HEIGHT), 3)\n",
    "        ),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        # Add a hidden layer with dropout\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        # Add an output layer with output units for all 6 gestures\n",
    "        tf.keras.layers.Dense(NUM_CATEGORIES, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Train neural network\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 5s 1s/step - loss: 25.2782 - accuracy: 0.2917\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 5s 1s/step - loss: 41.5758 - accuracy: 0.2359\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 5s 1s/step - loss: 3.9862 - accuracy: 0.2705\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 5s 1s/step - loss: 1.9700 - accuracy: 0.2548\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 5s 1s/step - loss: 2.1379 - accuracy: 0.3130\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 5s 1s/step - loss: 1.5886 - accuracy: 0.3896\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 5s 1s/step - loss: 1.4620 - accuracy: 0.4241\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 5s 1s/step - loss: 1.3116 - accuracy: 0.5008\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 5s 1s/step - loss: 1.2925 - accuracy: 0.4747\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 5s 1s/step - loss: 1.2346 - accuracy: 0.5783\n",
      "3/3 - 1s - loss: 1.4422 - accuracy: 0.3735\n",
      "NN fit time:  0:00:50.479795\n"
     ]
    }
   ],
   "source": [
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    np.array(images), np.array(labels), test_size=TEST_SIZE)\n",
    "\n",
    "# Get a compiled neural network\n",
    "model = get_model()\n",
    "\n",
    "# Fit model on training data\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=EPOCHS)\n",
    "\n",
    "# Evaluate neural network performance\n",
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "fitting_time = datetime.now()\n",
    "\n",
    "print(\"NN fit time: \", fitting_time - finish_loading_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.16639152 0.1660171  0.16711429 0.16586922 0.16718695 0.167421  ]]\n",
      "[5]\n",
      "[[0.16639996 0.16601337 0.16710094 0.16589278 0.16717643 0.16741647]]\n",
      "[5]\n",
      "[[0.1663832  0.16603966 0.16707848 0.16592325 0.1671748  0.16740058]]\n",
      "[5]\n",
      "[[0.1664158  0.16597441 0.16709417 0.16591911 0.16717906 0.16741747]]\n",
      "[5]\n",
      "[[0.16639572 0.16602601 0.16709104 0.16589946 0.16717578 0.16741204]]\n",
      "[5]\n",
      "[[0.166389   0.16601317 0.16709958 0.16590948 0.16717935 0.1674094 ]]\n",
      "[5]\n",
      "[[0.16639267 0.16599797 0.16709635 0.1659047  0.16718452 0.16742378]]\n",
      "[5]\n",
      "[[0.16640197 0.1659701  0.1670793  0.16591871 0.16718458 0.16744536]]\n",
      "[5]\n",
      "[[0.16640052 0.16597994 0.16708325 0.16589707 0.16719028 0.16744894]]\n",
      "[5]\n",
      "[[0.16639692 0.16597953 0.16708218 0.1659045  0.16719484 0.16744198]]\n",
      "[5]\n",
      "[[0.16638947 0.16598804 0.16709727 0.16590807 0.16719377 0.16742343]]\n",
      "[5]\n",
      "[[0.16639683 0.16598618 0.16709407 0.1659117  0.16719094 0.16742031]]\n",
      "[5]\n",
      "[[0.16638865 0.16600242 0.16709629 0.16592044 0.16718501 0.16740716]]\n",
      "[5]\n",
      "[[0.16639756 0.16599144 0.16708666 0.1659249  0.16718562 0.16741379]]\n",
      "[5]\n",
      "[[0.16640094 0.16597615 0.16710028 0.16591547 0.16719194 0.1674152 ]]\n",
      "[5]\n",
      "[[0.16641729 0.16595593 0.16709134 0.16590337 0.16719359 0.16743849]]\n",
      "[5]\n",
      "[[0.1664297  0.1659519  0.16709253 0.16589837 0.16718733 0.16744018]]\n",
      "[5]\n",
      "[[0.166432   0.16594538 0.16709164 0.1659057  0.16718642 0.16743883]]\n",
      "[5]\n",
      "[[0.16642746 0.16595258 0.16709772 0.1659097  0.167181   0.1674315 ]]\n",
      "[5]\n",
      "[[0.16643138 0.16595149 0.1670865  0.16592306 0.16717502 0.16743253]]\n",
      "[5]\n",
      "[[0.1664342  0.16595274 0.16708761 0.1659062  0.16718137 0.16743782]]\n",
      "[5]\n",
      "[[0.16643757 0.16594802 0.16709411 0.16590022 0.1671833  0.16743676]]\n",
      "[5]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "error",
     "evalue": "OpenCV(4.5.2) /tmp/pip-req-build-eirhwqtr/opencv/modules/highgui/src/window.cpp:404: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-22b791135fca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mhand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'video'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_addr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.2) /tmp/pip-req-build-eirhwqtr/opencv/modules/highgui/src/window.cpp:404: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n"
     ]
    }
   ],
   "source": [
    "video = cv2.VideoCapture(0)\n",
    "    \n",
    "while True:\n",
    "        # Capture the video frame\n",
    "        ret, img = video.read()\n",
    "\n",
    "         # Display the resulting frame\n",
    "        # to flip the video with 180 degree \n",
    "        image = cv2.flip(img, 1)\n",
    "        #cv2.imshow('frame', image)\n",
    "        \n",
    "        # save image for prediction\n",
    "        image = cv2.imwrite('Frame'+str(0)+'.jpg', image)\n",
    "        image_addr = \"Frame0.jpg\"\n",
    "        \n",
    "        handsModule = mediapipe.solutions.hands\n",
    "        with handsModule.Hands(static_image_mode=True) as hands:\n",
    "\n",
    "            image = cv2.imread(image_addr)\n",
    "            results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            image_height, image_width, _ = image.shape\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmark in results.multi_hand_landmarks:\n",
    "                    x = [landmark.x for landmark in hand_landmark.landmark]\n",
    "                    y = [landmark.y for landmark in hand_landmark.landmark]\n",
    "                \n",
    "                    center = np.array([np.mean(x)*image_width, np.mean(y)*image_height]).astype('int32')\n",
    "                    cv2.imshow('video', image)\n",
    "                    cv2.circle(image, tuple(center), 10, (255,0,0), 1) #for checking the center\n",
    "                    cv2.rectangle(image, (center[0]-128,center[1]-128), (center[0]+128,center[1]+128), (255,0,0), 1)\n",
    "                    hand = image[center[1]-128:center[1]+128, center[0]-128:center[0]+128]\n",
    "                #     cv2.imshow('video', hand)\n",
    "                    if hand.shape==(256, 256, 3):\n",
    "                        cv2.imwrite(image_addr, hand)\n",
    "       \n",
    "        # rest, thresh = cv2.threshold(image, 70, 255, cv2.THRESH_BINARY)\n",
    "        # _, contours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # image = cv2.imwrite(address, thresh)\n",
    "        # image = cv2.imwrite('Frame'+str(0)+'.png', image)\n",
    "\n",
    "        # image = \"Frame0.png\"\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        dim = (IMG_WIDTH, IMG_HEIGHT)\n",
    "        \n",
    "        image = tf.keras.preprocessing.image.load_img(image_addr, target_size=dim)\n",
    "        # Converts a PIL Image instance to a Numpy array. Return a 3D Numpy array.\n",
    "        input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        # Convert single image to a batch.\n",
    "        input_arr = np.array([input_arr])\n",
    "        input_arr = input_arr.astype('float32')/255\n",
    "        # Generates output predictions for the input samples. Return Numpy array(s) of predictions.\n",
    "        predictions = model.predict(input_arr)\n",
    "        print(predictions)\n",
    "        \n",
    "        # Return the index_array of the maximum values along an axis.\n",
    "        pre_class = np.argmax(predictions, axis=-1)\n",
    "        print(pre_class)\n",
    "        #print(GESTURE[pre_class[0]])\n",
    "        text = GESTURE[pre_class[0]]\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        image = cv2.flip(img, 1)\n",
    "        if predictions[0][pre_class[0]] >0.166: \n",
    "                cv2.putText(image, \n",
    "                        text, \n",
    "                        (50, 50), \n",
    "                        font, 2, \n",
    "                        (0, 0, 0), \n",
    "                        2, \n",
    "                        cv2.LINE_4)\n",
    "        cv2.imshow('video', image)\n",
    "    \n",
    "\n",
    "        # the 'q' button is set as the\n",
    "        # quitting button you may use any\n",
    "        # desired button of your choice\n",
    "        k = cv2.waitKey(1)\n",
    "        if k == ord('q'):\n",
    "                break\n",
    "\n",
    "video.release()       \n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import sys\n",
    "# import tensorflow as tf\n",
    "# from datetime import datetime\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.python.ops.gen_math_ops import mod\n",
    "\n",
    "# from PIL import Image, ImageFilter\n",
    "\n",
    "# EPOCHS = 10\n",
    "# IMG_WIDTH = 640\n",
    "# IMG_HEIGHT = 480\n",
    "# NUM_CATEGORIES = 6\n",
    "# TEST_SIZE = 0.5\n",
    "# GESTURE = {0:\"ok\", 1:\"down\", 2:\"up\", 3:\"palm\", 4:\"fist\", 5:\"l\"}\n",
    "\n",
    "# # Open image\n",
    "# def load_data(data_dir):\n",
    "#     \"\"\"\n",
    "#     Load image data from directory `data_dir`.\n",
    "#     Assume `data_dir` has one directory named after each category, numbered\n",
    "#     0 through NUM_CATEGORIES - 1. Inside each category directory will be some\n",
    "#     number of image files.\n",
    "#     Return tuple `(images, labels)`. `images` should be a list of all\n",
    "#     of the images in the data directory, where each image is formatted as a\n",
    "#     numpy ndarray with dimensions IMG_WIDTH x IMG_HEIGHT x 3. `labels` should\n",
    "#     be a list of integer labels, representing the categories for each of the\n",
    "#     corresponding `images`.\n",
    "#     \"\"\"\n",
    "#     images = []\n",
    "#     labels = []\n",
    "    \n",
    "#     for dir in range(0, NUM_CATEGORIES):\n",
    "#         # get path for each gesture like \"/home/arpine/Desktop/data/0\":  \n",
    "#         d = os.path.join(data_dir, f\"{str(dir)}\")\n",
    "#         # os.listdir(d) return the list of all names of images in that folder\n",
    "#         for image_path in os.listdir(d):\n",
    "#             # get the full path of specific image \n",
    "#             full_path = os.path.join(data_dir, f\"{str(dir)}\", image_path)\n",
    "#             image = Image.open(full_path).convert(\"RGB\")\n",
    "#             image = image.filter(ImageFilter.Kernel(\n",
    "#                 size=(3, 3),\n",
    "#                 kernel=[-1, -1, -1, -1, 8, -1, -1, -1, -1],\n",
    "#                 scale=1\n",
    "#             ))\n",
    "#             image.save(full_path)\n",
    "#             # Returns an image that is loaded from the specified file\n",
    "#             image = cv2.imread(full_path, )\n",
    "#             # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#             # cv2.imshow(\"im\", image)\n",
    "#             # get dimension for each image\n",
    "#             dim = (IMG_WIDTH, IMG_HEIGHT)\n",
    "#             # # resized the image\n",
    "#             image_resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "#              # add image and their directory name to images and labels list\n",
    "#             images.append(image_resized)\n",
    "#             labels.append(dir)\n",
    "    \n",
    "#     return images, labels\n",
    "#     # print(images)\n",
    "\n",
    "\n",
    "# # Filter image according to edge detection kernel\n",
    "# # image = image.filter(ImageFilter.Kernel(\n",
    "# #     size=(3, 3),\n",
    "# #     kernel=[-1, -1, -1, -1, 8, -1, -1, -1, -1],\n",
    "# #     scale=1\n",
    "# # ))\n",
    "\n",
    "# # # Show resulting image\n",
    "# # image.show()\n",
    "\n",
    "\n",
    "# # cv2.waitKey(0)\n",
    "# # cv2.destroyAllWindows()\n",
    "# load_data(\"/home/arpine/Desktop/Gesture/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# NUM_CATEGORIES = 6\n",
    "\n",
    "# GESTURE = {0:\"ok\", 1:\"down\", 2:\"up\", 3:\"palm\", 4:\"fist\", 5:\"l\"}\n",
    "\n",
    "# def load_data(data_dir):\n",
    "#     for dir in range(0, NUM_CATEGORIES):\n",
    "#     # adr = \"/home/arpine/Desktop/Gesture/poqr/ok/bcbf8425-d850-11eb-9ec6-0ba2456509ee.png\"\n",
    "#         d = os.path.join(data_dir, f\"{str(dir)}\")\n",
    "#         # os.listdir(d) return the list of all names of images in that folder\n",
    "#         for image_path in os.listdir(d):\n",
    "#         # get the full path of specific image \n",
    "#             full_path = os.path.join(data_dir, f\"{str(dir)}\", image_path)\n",
    "#             image = cv2.imread(full_path, 0)\n",
    "#             rest, thresh = cv2.threshold(image, 150, 255, cv2.THRESH_BINARY)\n",
    "#             _, contours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#             cv2.imwrite(full_path, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_path = \"/home/arpine/Desktop/Gesture/image/carmen/ok/000000014.jpg\"\n",
    "# image = cv2.imread(full_path, 0)\n",
    "# rest, thresh = cv2.threshold(image, 80, 80, cv2.THRESH_BINARY)\n",
    "# _, contours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# cv2.imwrite(\"frame.png\", thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}