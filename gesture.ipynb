{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.ops.gen_math_ops import mod\n",
    "from PIL import Image, ImageFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "NUM_CATEGORIES = 6\n",
    "TEST_SIZE = 0.5\n",
    "GESTURE = {0:\"ok\", 1:\"down\", 2:\"up\", 3:\"palm\", 4:\"fist\", 5:\"l\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    \"\"\"\n",
    "    Load image data from directory `data_dir`.\n",
    "    Assume `data_dir` has one directory named after each category, numbered\n",
    "    0 through NUM_CATEGORIES - 1. Inside each category directory will be some\n",
    "    number of image files.\n",
    "    Return tuple `(images, labels)`. `images` should be a list of all\n",
    "    of the images in the data directory, where each image is formatted as a\n",
    "    numpy ndarray with dimensions IMG_WIDTH x IMG_HEIGHT x 3. `labels` should\n",
    "    be a list of integer labels, representing the categories for each of the\n",
    "    corresponding `images`.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for dir in range(0, NUM_CATEGORIES):\n",
    "        # get path for each gesture like \"/home/arpine/Desktop/data/0\":  \n",
    "        d = os.path.join(data_dir, f\"{str(dir)}\")\n",
    "        # os.listdir(d) return the list of all names of images in that folder\n",
    "        for image_path in os.listdir(d):\n",
    "            # get the full path of specific image \n",
    "            full_path = os.path.join(data_dir, f\"{str(dir)}\", image_path)\n",
    "            # Returns an image that is loaded from the specified file\n",
    "            image = cv2.imread(full_path, )\n",
    "            # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            # cv2.imshow(\"im\", image)\n",
    "            # get dimension for each image\n",
    "            dim = (IMG_WIDTH, IMG_HEIGHT)\n",
    "            # resized the image\n",
    "            image_resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "            # add image and their directory name to images and labels list\n",
    "            images.append(image_resized)\n",
    "            labels.append(dir)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading ===========\n",
      "Images load time:  0:00:00.951040\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()  \n",
    "print(\"Loading ===========\")  \n",
    "images, labels = load_data(\"/home/arpine/Desktop/Gesture/poqr\")  \n",
    "finish_loading_time = datetime.now()\n",
    "print(\"Images load time: \", finish_loading_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"\n",
    "    Returns a compiled convolutional neural network model. Assume that the\n",
    "    `input_shape` of the first layer is `(IMG_WIDTH, IMG_HEIGHT, 3)`.\n",
    "    The output layer should have `NUM_CATEGORIES` units, one for each category.\n",
    "    \"\"\"\n",
    "    # Create a convolutional neural network\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "        # Convolutional layer. Learn 32 filters using a 3x3 kernel\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32, (5, 5), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "        ),\n",
    "        # Max-pooling layer, using 2x2 pool size\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            64, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "        ),\n",
    "        # Max-pooling layer, using 2x2 pool size\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            128, (3, 3), activation='relu', input_shape=((IMG_WIDTH), (IMG_HEIGHT), 3)\n",
    "        ),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            256, (3, 3), activation='relu', input_shape=((IMG_WIDTH), (IMG_HEIGHT), 3)\n",
    "        ),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        # Add a hidden layer with dropout\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        #tf.keras.layers.Dropout(0.3),\n",
    "        # Add an output layer with output units for all 6 gestures\n",
    "        tf.keras.layers.Dense(NUM_CATEGORIES, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Train neural network\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "21/21 [==============================] - 55s 3s/step - loss: 34.3979 - accuracy: 0.6501\n",
      "Epoch 2/3\n",
      "21/21 [==============================] - 56s 3s/step - loss: 0.0092 - accuracy: 0.9992\n",
      "Epoch 3/3\n",
      "21/21 [==============================] - 56s 3s/step - loss: 0.0019 - accuracy: 1.0000\n",
      "41/41 - 11s - loss: 0.0093 - accuracy: 0.9977\n",
      "NN fit time:  0:03:04.191403\n"
     ]
    }
   ],
   "source": [
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    np.array(images), np.array(labels), test_size=TEST_SIZE)\n",
    "\n",
    "# Get a compiled neural network\n",
    "model = get_model()\n",
    "\n",
    "# Fit model on training data\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=EPOCHS)\n",
    "\n",
    "# Evaluate neural network performance\n",
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "fitting_time = datetime.now()\n",
    "\n",
    "print(\"NN fit time: \", fitting_time - finish_loading_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "error",
     "evalue": "OpenCV(4.5.2) /tmp/pip-req-build-eirhwqtr/opencv/modules/imgcodecs/src/loadsave.cpp:721: error: (-215:Assertion failed) !_img.empty() in function 'imwrite'\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-9a22fd726e80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# address = \"/home/arpine/Desktop/Gesture/Frame0.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# image = cv2.imwrite(address, image)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Frame'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Frame0.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.2) /tmp/pip-req-build-eirhwqtr/opencv/modules/imgcodecs/src/loadsave.cpp:721: error: (-215:Assertion failed) !_img.empty() in function 'imwrite'\n"
     ]
    }
   ],
   "source": [
    "video = cv2.VideoCapture(0)\n",
    "    \n",
    "while True:\n",
    "        # Capture the video frame\n",
    "        ret, img = video.read()\n",
    "\n",
    "        # Display the resulting frame\n",
    "        # to flip the video with 180 degree \n",
    "        image = cv2.flip(img, 1)\n",
    "        \n",
    "        # address = \"/home/arpine/Desktop/Gesture/Frame0.png\"\n",
    "        # image = cv2.imwrite(address, image)\n",
    "        image = cv2.imwrite('Frame'+str(0)+'.png', image)\n",
    "        image_path = \"Frame0.png\"\n",
    "        image = cv2.imread(image_path, 0)\n",
    "        # save image for prediction\n",
    "        # image = cv2.imread(image, 0)\n",
    "        rest, thresh = cv2.threshold(image, 70, 255, cv2.THRESH_BINARY)\n",
    "        _, contours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        image = cv2.imwrite(image_path, thresh)\n",
    "        # image = cv2.imwrite('Frame'+str(0)+'.png', image)\n",
    "        # address = \"/home/arpine/Desktop/Gesture/Frame0.png\"\n",
    "        # image = Image.open(address).convert(\"RGB\")\n",
    "        # image = image.filter(ImageFilter.Kernel(\n",
    "        #         size=(3, 3),\n",
    "        #         kernel=[-1, -1, -1, -1, 8, -1, -1, -1, -1],\n",
    "        #         scale=1\n",
    "        #     ))\n",
    "        # image = image.save(address)\n",
    "        # image = \"Frame0.png\"\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        dim = (IMG_WIDTH, IMG_HEIGHT)\n",
    "        \n",
    "        image = tf.keras.preprocessing.image.load_img(image, target_size=dim)\n",
    "        # Converts a PIL Image instance to a Numpy array. Return a 3D Numpy array.\n",
    "        input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        # Convert single image to a batch.\n",
    "        input_arr = np.array([input_arr])\n",
    "        input_arr = input_arr.astype('float32')/255\n",
    "        # Generates output predictions for the input samples. Return Numpy array(s) of predictions.\n",
    "        predictions = model.predict(input_arr)\n",
    "        # Return the index_array of the maximum values along an axis.\n",
    "        pre_class = np.argmax(predictions, axis=-1)\n",
    "        #print(GESTURE[pre_class[0]])\n",
    "        text = GESTURE[pre_class[0]]\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        image = cv2.flip(img, 1)\n",
    "        cv2.putText(image, \n",
    "                text, \n",
    "                (50, 50), \n",
    "                font, 2, \n",
    "                (0, 0, 0), \n",
    "                2, \n",
    "                cv2.LINE_4)\n",
    "        cv2.imshow('video', image)\n",
    "    \n",
    "\n",
    "        # the 'q' button is set as the\n",
    "        # quitting button you may use any\n",
    "        # desired button of your choice\n",
    "        k = cv2.waitKey(0)\n",
    "        # if cv2.waitKey(10) and 0xFF == ord('q'):\n",
    "        if k == ord('q'):\n",
    "                break\n",
    "        #     break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "video.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import sys\n",
    "# import tensorflow as tf\n",
    "# from datetime import datetime\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.python.ops.gen_math_ops import mod\n",
    "\n",
    "# from PIL import Image, ImageFilter\n",
    "\n",
    "# EPOCHS = 10\n",
    "# IMG_WIDTH = 640\n",
    "# IMG_HEIGHT = 480\n",
    "# NUM_CATEGORIES = 6\n",
    "# TEST_SIZE = 0.5\n",
    "# GESTURE = {0:\"ok\", 1:\"down\", 2:\"up\", 3:\"palm\", 4:\"fist\", 5:\"l\"}\n",
    "\n",
    "# # Open image\n",
    "# def load_data(data_dir):\n",
    "#     \"\"\"\n",
    "#     Load image data from directory `data_dir`.\n",
    "#     Assume `data_dir` has one directory named after each category, numbered\n",
    "#     0 through NUM_CATEGORIES - 1. Inside each category directory will be some\n",
    "#     number of image files.\n",
    "#     Return tuple `(images, labels)`. `images` should be a list of all\n",
    "#     of the images in the data directory, where each image is formatted as a\n",
    "#     numpy ndarray with dimensions IMG_WIDTH x IMG_HEIGHT x 3. `labels` should\n",
    "#     be a list of integer labels, representing the categories for each of the\n",
    "#     corresponding `images`.\n",
    "#     \"\"\"\n",
    "#     images = []\n",
    "#     labels = []\n",
    "    \n",
    "#     for dir in range(0, NUM_CATEGORIES):\n",
    "#         # get path for each gesture like \"/home/arpine/Desktop/data/0\":  \n",
    "#         d = os.path.join(data_dir, f\"{str(dir)}\")\n",
    "#         # os.listdir(d) return the list of all names of images in that folder\n",
    "#         for image_path in os.listdir(d):\n",
    "#             # get the full path of specific image \n",
    "#             full_path = os.path.join(data_dir, f\"{str(dir)}\", image_path)\n",
    "#             image = Image.open(full_path).convert(\"RGB\")\n",
    "#             image = image.filter(ImageFilter.Kernel(\n",
    "#                 size=(3, 3),\n",
    "#                 kernel=[-1, -1, -1, -1, 8, -1, -1, -1, -1],\n",
    "#                 scale=1\n",
    "#             ))\n",
    "#             image.save(full_path)\n",
    "#             # Returns an image that is loaded from the specified file\n",
    "#             image = cv2.imread(full_path, )\n",
    "#             # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#             # cv2.imshow(\"im\", image)\n",
    "#             # get dimension for each image\n",
    "#             dim = (IMG_WIDTH, IMG_HEIGHT)\n",
    "#             # # resized the image\n",
    "#             image_resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "#              # add image and their directory name to images and labels list\n",
    "#             images.append(image_resized)\n",
    "#             labels.append(dir)\n",
    "    \n",
    "#     return images, labels\n",
    "#     # print(images)\n",
    "\n",
    "\n",
    "# # Filter image according to edge detection kernel\n",
    "# # image = image.filter(ImageFilter.Kernel(\n",
    "# #     size=(3, 3),\n",
    "# #     kernel=[-1, -1, -1, -1, 8, -1, -1, -1, -1],\n",
    "# #     scale=1\n",
    "# # ))\n",
    "\n",
    "# # # Show resulting image\n",
    "# # image.show()\n",
    "\n",
    "\n",
    "# # cv2.waitKey(0)\n",
    "# # cv2.destroyAllWindows()\n",
    "# load_data(\"/home/arpine/Desktop/Gesture/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "NUM_CATEGORIES = 6\n",
    "\n",
    "GESTURE = {0:\"ok\", 1:\"down\", 2:\"up\", 3:\"palm\", 4:\"fist\", 5:\"l\"}\n",
    "\n",
    "def load_data(data_dir):\n",
    "    for dir in range(0, NUM_CATEGORIES):\n",
    "    # adr = \"/home/arpine/Desktop/Gesture/poqr/ok/bcbf8425-d850-11eb-9ec6-0ba2456509ee.png\"\n",
    "        d = os.path.join(data_dir, f\"{str(dir)}\")\n",
    "        # os.listdir(d) return the list of all names of images in that folder\n",
    "        for image_path in os.listdir(d):\n",
    "        # get the full path of specific image \n",
    "            full_path = os.path.join(data_dir, f\"{str(dir)}\", image_path)\n",
    "            image = cv2.imread(full_path, 0)\n",
    "            rest, thresh = cv2.threshold(image, 150, 255, cv2.THRESH_BINARY)\n",
    "            _, contours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.imwrite(full_path, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data(\"/home/arpine/Desktop/Gesture/poqr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}