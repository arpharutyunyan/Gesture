{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import preprocessing\n",
    "# from tensorflow.python.ops.gen_math_ops import mod\n",
    "# from PIL import Image, ImageFilter\n",
    "# import mediapipe\n",
    "from matplotlib import pyplot as plt\n",
    "# from plot_keras_history import plot_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 150\n",
    "NUM_CATEGORIES = 5\n",
    "# TEST_SIZE = 0.4\n",
    "GESTURE = {0:\"ok\", 1:\"up\", 2:\"down\", 3:\"palm\", 4:\"l\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    \"\"\"\n",
    "    Load image data from directory `data_dir`.\n",
    "    Assume `data_dir` has one directory named after each category, numbered\n",
    "    0 through NUM_CATEGORIES - 1. Inside each category directory will be some\n",
    "    number of image files.\n",
    "    Return tuple `(images, labels)`. `images` should be a list of all\n",
    "    of the images in the data directory, where each image is formatted as a\n",
    "    numpy ndarray with dimensions IMG_WIDTH x IMG_HEIGHT x 3. `labels` should\n",
    "    be a list of integer labels, representing the categories for each of the\n",
    "    corresponding `images`.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for dir in range(0, NUM_CATEGORIES):\n",
    "        # get path for each gesture like \"/home/arpine/Desktop/data/0\":  \n",
    "        d = os.path.join(data_dir, f\"{str(dir)}\")\n",
    "        # os.listdir(d) return the list of all names of images in that folder\n",
    "        for image_path in os.listdir(d):\n",
    "            # get the full path of specific image \n",
    "            full_path = os.path.join(data_dir, f\"{str(dir)}\", image_path)\n",
    "            # Returns an image that is loaded from the specified file\n",
    "            image = cv2.imread(full_path, )\n",
    "        #     print(image.shape)\n",
    "        #     image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        #     print(image.shape)\n",
    "            # cv2.imshow(\"im\", image)\n",
    "            # get dimension for each image\n",
    "            dim = (IMG_WIDTH, IMG_HEIGHT)\n",
    "            # resized the image\n",
    "            image_resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "        #     print(image_resized.shape)\n",
    "            \n",
    "        #     add image and their directory name to images and labels list\n",
    "            images.append(image_resized)\n",
    "            labels.append(dir)\n",
    "    # images = np.load(images)\n",
    "    # labels = np.load(labels)\n",
    "\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading ===========\n",
      "Images load time:  0:03:55.789620\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()  \n",
    "print(\"Loading ===========\") \n",
    "images_test, labels_test = load_data(\"/home/arpine/Desktop/Gesture/test\") \n",
    "images_train, labels_train = load_data(\"/home/arpine/Desktop/Gesture/train\") \n",
    "valid_image, valid_label  = load_data(\"/home/arpine/Desktop/Gesture/valid\")\n",
    "\n",
    "# images_test, labels_test = load_data(\"images_test.npy\", \"labels_test.npy\") \n",
    "# images_train, labels_train = load_data(\"images_train.npy\", \"labels_train.npy\" ) \n",
    "# valid_image, valid_label  = load_data(\"images_valid.npy\", \"labels_valid.npy\" )\n",
    "finish_loading_time = datetime.now()\n",
    "print(\"Images load time: \", finish_loading_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"\n",
    "    Returns a compiled convolutional neural network model. Assume that the\n",
    "    `input_shape` of the first layer is `(IMG_WIDTH, IMG_HEIGHT, 3)`.\n",
    "    The output layer should have `NUM_CATEGORIES` units, one for each category.\n",
    "    \"\"\"\n",
    "    # Create a convolutional neural network\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "        # Convolutional layer. Learn 32 filters using a 3x3 kernel\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32, (5, 5), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "        ),\n",
    "        # Max-pooling layer, using 2x2 pool size\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            64, (3, 3), activation='relu' #, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "        ),\n",
    "        # Max-pooling layer, using 2x2 pool size\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        # tf.keras.layers.Conv2D(\n",
    "        #     64, (3, 3), activation='relu' #, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "        # ),\n",
    "        # # Max-pooling layer, using 2x2 pool size\n",
    "        # tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            128, (3, 3), activation='relu' #, input_shape=((IMG_WIDTH), (IMG_HEIGHT), 3)\n",
    "        ),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        # tf.keras.layers.Conv2D(\n",
    "        #     128, (3, 3), activation='relu' #, input_shape=((IMG_WIDTH), (IMG_HEIGHT), 3)\n",
    "        # ),\n",
    "        # tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            256, (3, 3), activation='relu'#, input_shape=((IMG_WIDTH), (IMG_HEIGHT), 3)\n",
    "        ),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        # Add a hidden layer with dropout\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        # tf.keras.layers.Dense(128, activation='relu'),\n",
    "        # tf.keras.layers.Dropout(0.3),\n",
    "        # Add an output layer with output units for all 6 gestures\n",
    "        # tf.keras.layers.Dense(NUM_CATEGORIES, activation='sigmoid')\n",
    "        tf.keras.layers.Dense(NUM_CATEGORIES, activation='softmax')\n",
    "    ])\n",
    "\n",
    "  \n",
    "    # optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    # model.compile(\n",
    "    #     optimizer=optimizer ,\n",
    "    #     loss=\"categorical_crossentropy\",\n",
    "    #     metrics=[\"accuracy\"]\n",
    "    # )\n",
    "    # return model\n",
    "\n",
    "      # Train neural network\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "print(\"====================\")\n",
    "y_test = tf.keras.utils.to_categorical(labels_test)\n",
    "y_train = tf.keras.utils.to_categorical(labels_train)\n",
    "y_valid = tf.keras.utils.to_categorical(valid_label)\n",
    "\n",
    "x_train = np.array(images_train)/255\n",
    "\n",
    "x_test = np.array(images_test)/255\n",
    "# print(x_test)\n",
    "x_valid = np.array(valid_image)/255\n",
    "\n",
    "# x_test_min = np.array(images_test).min(axis=(1, 2), keepdims=True)\n",
    "# x_test_max = np.array(images_test).max(axis=(1, 2), keepdims=True)\n",
    "\n",
    "# x_test = (np.array(images_test) - x_test_min) / (x_test_max - x_test_min)\n",
    "# print(x_test)\n",
    "\n",
    "# x_train_min = np.array(images_train).min(axis=(1, 2), keepdims=True)\n",
    "# x_train_max = np.array(images_train).max(axis=(1, 2), keepdims=True)\n",
    "\n",
    "# x_train = (np.array(images_train) - x_train_min) / (x_train_max - x_train_min)\n",
    "# print(x_train.shape)\n",
    "\n",
    "\n",
    "# x_valid_min = np.array(valid_image).min(axis=(1, 2), keepdims=True)\n",
    "# x_valid_max = np.array(valid_image).max(axis=(1, 2), keepdims=True)\n",
    "\n",
    "# x_valid = (np.array(valid_image) - x_valid_min) / (x_valid_max - x_valid_min)\n",
    "# print(x_valid.shape)\n",
    "\n",
    "\n",
    "print(\"====================\")\n",
    "# Get a compiled neural network\n",
    "fitting_time = datetime.now()\n",
    "model = get_model()\n",
    "\n",
    "# Fit model on training data\n",
    "history = model.fit(x_train, y_train, epochs=EPOCHS, validation_data = (x_valid, y_valid))\n",
    "# model.fit(train_data, epochs= EPOCHS)\n",
    "# Evaluate neural network performance\n",
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "finish_loading_time = datetime.now()\n",
    "\n",
    "print(\"NN fit time: \", datetime.now() - fitting_time)\n",
    "print(history.history.keys())\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['y_train', 'y_test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['y_train', 'y_test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# x_test = preprocessing.normalize(np.array(images_test))\n",
    "# x_train = preprocessing.normalize(np.array(images_train))\n",
    "# x_valid = preprocessing.normalize(np.array(valid_image))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a compiled neural network\n",
    "# fitting_time = datetime.now()\n",
    "# model = get_model()\n",
    "\n",
    "# # Fit model on training data\n",
    "# history = model.fit(x_train, y_train, epochs=EPOCHS, validation_data = (x_valid, y_valid))\n",
    "# # model.fit(train_data, epochs= EPOCHS)\n",
    "# # Evaluate neural network performance\n",
    "# model.evaluate(x_test, y_test, verbose=2)\n",
    "# finish_loading_time = datetime.now()\n",
    "\n",
    "# print(\"NN fit time: \", datetime.now() - fitting_time)\n",
    "# print(history.history.keys())\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['y_train', 'y_test'], loc='upper left')\n",
    "# plt.show()\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['y_train', 'y_test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(0)\n",
    "    \n",
    "while True:\n",
    "        # Capture the video frame\n",
    "        ret, img = video.read()\n",
    "\n",
    "         # Display the resulting frame\n",
    "        # to flip the video with 180 degree \n",
    "        image = cv2.flip(img, 1)\n",
    "        #cv2.imshow('frame', image)\n",
    "        \n",
    "        # save image for prediction\n",
    "        image = cv2.imwrite('Frame'+str(0)+'.jpg', image)\n",
    "        image_addr = \"Frame0.jpg\"\n",
    "        # image = cv2.imread(image_addr)\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # cv2.imshow(\"gray\", image)\n",
    "        # handsModule = mediapipe.solutions.hands\n",
    "        # with handsModule.Hands(static_image_mode=True) as hands:\n",
    "\n",
    "        #         image = cv2.imread(image_addr)\n",
    "        #         results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        #         image_height, image_width, _ = image.shape\n",
    "\n",
    "        #         if results.multi_hand_landmarks:\n",
    "        #                 for hand_landmark in results.multi_hand_landmarks:\n",
    "        #                         x = [landmark.x for landmark in hand_landmark.landmark]\n",
    "        #                         y = [landmark.y for landmark in hand_landmark.landmark]\n",
    "\n",
    "        #                         center = np.array([np.mean(x)*image_width, np.mean(y)*image_height]).astype('int32')\n",
    "        #                 #     cv2.imshow('video', image)\n",
    "        #                         cv2.circle(image, tuple(center), 10, (255,0,0), 1) #for checking the center\n",
    "        #                         cv2.rectangle(image, (center[0]-128,center[1]-128), (center[0]+128,center[1]+128), (255,0,0), 1)\n",
    "        #                         hand = image[center[1]-128:center[1]+128, center[0]-128:center[0]+128]\n",
    "        #                         # hand = cv2.resize(hand, dim, interpolation = cv2.INTER_AREA)\n",
    "        #                 #     cv2.imshow('video', hand)\n",
    "        #                         if hand.shape==(256, 256, 3):\n",
    "\n",
    "        #                                 image_hand = cv2.imwrite(image_addr, hand)\n",
    "\n",
    "        # rest, thresh = cv2.threshold(image, 70, 255, cv2.THRESH_BINARY)\n",
    "        # _, contours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # image = cv2.imwrite(address, thresh)\n",
    "        # image = cv2.imwrite('Frame'+str(0)+'.png', image)\n",
    "\n",
    "        # image = \"Frame0.png\"\n",
    "        # image = cv2.cvtColor(image_addr, cv2.COLOR_BGR2GRAY)\n",
    "        dim = (IMG_WIDTH, IMG_HEIGHT)\n",
    "        \n",
    "        # image = tf.keras.preprocessing.image.load_img(image_addr, color_mode=\"grayscale\", target_size=dim)\n",
    "        image = tf.keras.preprocessing.image.load_img(image_addr, target_size=dim)\n",
    "\n",
    "        # Converts a PIL Image instance to a Numpy array. Return a 3D Numpy array.\n",
    "        input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        # Convert single image to a batch.\n",
    "        input_arr = np.array([input_arr])\n",
    "        input_arr = input_arr.astype('float32')/255\n",
    "        # input_arr = input_arr.astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "        # predict_min = input_arr.min(axis=(1, 2), keepdims=True)\n",
    "        # predict_max = input_arr.max(axis=(1, 2), keepdims=True)\n",
    "\n",
    "        # x_predict = (input_arr - predict_min) / (predict_max - predict_min)\n",
    "\n",
    "\n",
    "\n",
    "        # Generates output predictions for the input samples. Return Numpy array(s) of predictions.\n",
    "        predictions = model.predict(input_arr)\n",
    "        print(predictions)\n",
    "        \n",
    "        # Return the index_array of the maximum values along an axis.\n",
    "        pre_class = np.argmax(predictions, axis=-1)\n",
    "        print(pre_class)\n",
    "        #print(GESTURE[pre_class[0]])\n",
    "        text = GESTURE[pre_class[0]]\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        image = cv2.flip(img, 1)\n",
    "        if predictions[0][pre_class[0]] > 0.: \n",
    "                cv2.putText(image, \n",
    "                        text, \n",
    "                        (50, 50), \n",
    "                        font, 2,\n",
    "                        (0, 0, 0), \n",
    "                        2, \n",
    "                        cv2.LINE_4)\n",
    "        cv2.imshow('video', image)\n",
    "    \n",
    "\n",
    "        # the 'q' button is set as the\n",
    "        # quitting button you may use any\n",
    "        # desired button of your choice\n",
    "        k = cv2.waitKey(1)\n",
    "        if k == ord('q'):\n",
    "                break\n",
    "\n",
    "video.release()       \n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import sys\n",
    "# import tensorflow as tf\n",
    "# from datetime import datetime\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.python.ops.gen_math_ops import mod\n",
    "\n",
    "# from PIL import Image, ImageFilter\n",
    "\n",
    "# EPOCHS = 10\n",
    "# IMG_WIDTH = 640\n",
    "# IMG_HEIGHT = 480\n",
    "# NUM_CATEGORIES = 6\n",
    "# TEST_SIZE = 0.5\n",
    "# GESTURE = {0:\"ok\", 1:\"down\", 2:\"up\", 3:\"palm\", 4:\"fist\", 5:\"l\"}\n",
    "\n",
    "# # Open image\n",
    "# def load_data(data_dir):\n",
    "#     \"\"\"\n",
    "#     Load image data from directory `data_dir`.\n",
    "#     Assume `data_dir` has one directory named after each category, numbered\n",
    "#     0 through NUM_CATEGORIES - 1. Inside each category directory will be some\n",
    "#     number of image files.\n",
    "#     Return tuple `(images, labels)`. `images` should be a list of all\n",
    "#     of the images in the data directory, where each image is formatted as a\n",
    "#     numpy ndarray with dimensions IMG_WIDTH x IMG_HEIGHT x 3. `labels` should\n",
    "#     be a list of integer labels, representing the categories for each of the\n",
    "#     corresponding `images`.\n",
    "#     \"\"\"\n",
    "#     images = []\n",
    "#     labels = []\n",
    "    \n",
    "#     for dir in range(0, NUM_CATEGORIES):\n",
    "#         # get path for each gesture like \"/home/arpine/Desktop/data/0\":  \n",
    "#         d = os.path.join(data_dir, f\"{str(dir)}\")\n",
    "#         # os.listdir(d) return the list of all names of images in that folder\n",
    "#         for image_path in os.listdir(d):\n",
    "#             # get the full path of specific image \n",
    "#             full_path = os.path.join(data_dir, f\"{str(dir)}\", image_path)\n",
    "#             image = Image.open(full_path).convert(\"RGB\")\n",
    "#             image = image.filter(ImageFilter.Kernel(\n",
    "#                 size=(3, 3),\n",
    "#                 kernel=[-1, -1, -1, -1, 8, -1, -1, -1, -1],\n",
    "#                 scale=1\n",
    "#             ))\n",
    "#             image.save(full_path)\n",
    "#             # Returns an image that is loaded from the specified file\n",
    "#             image = cv2.imread(full_path, )\n",
    "#             # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#             # cv2.imshow(\"im\", image)\n",
    "#             # get dimension for each image\n",
    "#             dim = (IMG_WIDTH, IMG_HEIGHT)\n",
    "#             # # resized the image\n",
    "#             image_resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "#              # add image and their directory name to images and labels list\n",
    "#             images.append(image_resized)\n",
    "#             labels.append(dir)\n",
    "    \n",
    "#     return images, labels\n",
    "#     # print(images)\n",
    "\n",
    "\n",
    "# # Filter image according to edge detection kernel\n",
    "# # image = image.filter(ImageFilter.Kernel(\n",
    "# #     size=(3, 3),\n",
    "# #     kernel=[-1, -1, -1, -1, 8, -1, -1, -1, -1],\n",
    "# #     scale=1\n",
    "# # ))\n",
    "\n",
    "# # # Show resulting image\n",
    "# # image.show()\n",
    "\n",
    "\n",
    "# # cv2.waitKey(0)\n",
    "# # cv2.destroyAllWindows()\n",
    "# load_data(\"/home/arpine/Desktop/Gesture/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# NUM_CATEGORIES = 6\n",
    "\n",
    "# GESTURE = {0:\"ok\", 1:\"down\", 2:\"up\", 3:\"palm\", 4:\"fist\", 5:\"l\"}\n",
    "\n",
    "# def load_data(data_dir):\n",
    "#     for dir in range(0, NUM_CATEGORIES):\n",
    "#     # adr = \"/home/arpine/Desktop/Gesture/poqr/ok/bcbf8425-d850-11eb-9ec6-0ba2456509ee.png\"\n",
    "#         d = os.path.join(data_dir, f\"{str(dir)}\")\n",
    "#         # os.listdir(d) return the list of all names of images in that folder\n",
    "#         for image_path in os.listdir(d):\n",
    "#         # get the full path of specific image \n",
    "#             full_path = os.path.join(data_dir, f\"{str(dir)}\", image_path)\n",
    "#             image = cv2.imread(full_path, 0)\n",
    "#             rest, thresh = cv2.threshold(image, 150, 255, cv2.THRESH_BINARY)\n",
    "#             _, contours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#             cv2.imwrite(full_path, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_path = \"/home/arpine/Desktop/Gesture/image/carmen/ok/000000014.jpg\"\n",
    "# image = cv2.imread(full_path, 0)\n",
    "# rest, thresh = cv2.threshold(image, 80, 80, cv2.THRESH_BINARY)\n",
    "# _, contours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# cv2.imwrite(\"frame.png\", thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}